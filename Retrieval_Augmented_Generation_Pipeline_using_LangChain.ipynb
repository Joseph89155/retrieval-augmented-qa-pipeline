{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKAU7tPevTbkPI4G7IvG59",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joseph89155/retrieval-augmented-qa-pipeline/blob/main/Retrieval_Augmented_Generation_Pipeline_using_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠🔍 Generative AI and RAG (Retrieval-Augmented Generation) Project\n",
        "\n",
        "## 📌 Overview\n",
        "In this project, we explore **Generative AI** integrated with **Retrieval-Augmented Generation (RAG)** to build an intelligent question-answering pipeline. The system can retrieve relevant chunks from a document and generate context-aware answers using a Large Language Model (LLM). Unlike traditional QA systems, this approach ensures that responses are grounded in actual content rather than hallucinated information.\n",
        "\n",
        "## 🎯 Objectives\n",
        "- To extract and store key information from a PDF document using **chunking** and **semantic embeddings**\n",
        "- To use **FAISS**, a vector similarity search library, for fast and efficient retrieval\n",
        "- To integrate **generative models** using Hugging Face Transformers and LangChain for creating detailed answers\n",
        "- To compare **document-grounded answers** with generic ones and highlight the benefits of RAG\n",
        "- To apply **prompt engineering** techniques for guiding LLMs toward clearer and more relevant responses\n",
        "\n",
        "## 🧩 Key Technologies Used\n",
        "- **Python** (Google Colab environment)\n",
        "- **LangChain** for chaining LLMs and retrieval components\n",
        "- **Sentence-Transformers** for creating vector embeddings\n",
        "- **FAISS** for similarity search\n",
        "- **Hugging Face Transformers** for using pre-trained LLMs\n",
        "- **PyMuPDF (fitz)** or **pdfplumber** for PDF parsing\n",
        "\n",
        "## 🌍 Real-World Relevance\n",
        "RAG pipelines are increasingly important in real-world NLP applications:\n",
        "- **Customer support chatbots** grounded in documentation\n",
        "- **Legal and compliance assistants** referencing internal policies\n",
        "- **Medical assistants** referring to health records and guidelines\n",
        "- **Enterprise knowledge bases** for internal document search and summarization\n",
        "\n",
        "By completing this project, we simulate how modern AI solutions use internal knowledge sources to produce accurate, transparent, and grounded answers — a growing demand in regulated and high-stakes environments.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "KJfjyvL3iMhX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📄 Step 1: Uploading and Reading the PDF\n",
        "\n",
        "We begin by uploading our document  *\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"*  which introduces the RAG architecture. This foundational paper will act as our knowledge source for querying. We'll load the PDF, extract its text, and inspect the content before chunking it into retrievable segments.\n"
      ],
      "metadata": {
        "id": "qJa2qO54iTZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required library\n",
        "!pip install -q PyMuPDF\n",
        "\n",
        "# Import necessary modules\n",
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "\n",
        "# Load the PDF file\n",
        "pdf_path = \"2005.11401v4.pdf\"  # Ensure this matches your uploaded filename\n",
        "doc = fitz.open(pdf_path)\n",
        "\n",
        "# Extract all text\n",
        "all_text = \"\"\n",
        "for page in doc:\n",
        "    all_text += page.get_text()\n",
        "\n",
        "# Display a sample of the text\n",
        "print(all_text[:3000])  # Show first 3000 characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Giv_Du9ai9rN",
        "outputId": "7ae5a7ec-ecab-4aa0-d314-c489638e0d57"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval-Augmented Generation for\n",
            "Knowledge-Intensive NLP Tasks\n",
            "Patrick Lewis†‡, Ethan Perez⋆,\n",
            "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
            "Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n",
            "†Facebook AI Research; ‡University College London; ⋆New York University;\n",
            "plewis@fb.com\n",
            "Abstract\n",
            "Large pre-trained language models have been shown to store factual knowledge\n",
            "in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\n",
            "stream NLP tasks. However, their ability to access and precisely manipulate knowl-\n",
            "edge is still limited, and hence on knowledge-intensive tasks, their performance\n",
            "lags behind task-speciﬁc architectures. Additionally, providing provenance for their\n",
            "decisions and updating their world knowledge remain open research problems. Pre-\n",
            "trained models with a differentiable access mechanism to explicit non-parametric\n",
            "memory have so far been only investigated for extractive downstream tasks. We\n",
            "explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\n",
            "(RAG) — models which combine pre-trained parametric and non-parametric mem-\n",
            "ory for language generation. We introduce RAG models where the parametric\n",
            "memory is a pre-trained seq2seq model and the non-parametric memory is a dense\n",
            "vector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\n",
            "pare two RAG formulations, one which conditions on the same retrieved passages\n",
            "across the whole generated sequence, and another which can use different passages\n",
            "per token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\n",
            "intensive NLP tasks and set the state of the art on three open domain QA tasks,\n",
            "outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\n",
            "architectures. For language generation tasks, we ﬁnd that RAG models generate\n",
            "more speciﬁc, diverse and factual language than a state-of-the-art parametric-only\n",
            "seq2seq baseline.\n",
            "1\n",
            "Introduction\n",
            "Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\n",
            "edge from data [47]. They can do so without any access to an external memory, as a parameterized\n",
            "implicit knowledge base [51, 52]. While this development is exciting, such models do have down-\n",
            "sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\n",
            "their predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric\n",
            "memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\n",
            "issues because knowledge can be directly revised and expanded, and accessed knowledge can be\n",
            "inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that\n",
            "combine masked language models [8] with a differentiable retriever, have shown promising results,\n",
            "arXiv:2005.11401v4  [cs.CL]  12 Apr 2021\n",
            "The Divine\n",
            "Comedy (x)\n",
            "q\n",
            "Query\n",
            "Encoder\n",
            "q(x)\n",
            "MIPS\n",
            "pθ\n",
            "Generator pθ\n",
            "(Parametric)\n",
            "Margin-\n",
            "alize\n",
            "This 14th \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Splitting the Document into Chunks\n",
        "\n",
        "To enable semantic search and retrieval, we split the full document into smaller text chunks. These chunks provide the retriever with granular segments of the document, allowing the system to identify and return the most relevant context for any query.\n",
        "\n",
        "We use a **sliding window approach** with slight overlaps to maintain semantic continuity across chunks. Each chunk will be embedded and stored in the FAISS vector store during the next step.\n"
      ],
      "metadata": {
        "id": "6BHqebKYj3qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import re\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    # Basic cleaning to remove excessive whitespace and line breaks\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def split_into_chunks(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
        "    \"\"\"\n",
        "    Splits text into overlapping chunks using a sliding window approach.\n",
        "    :param text: The full document text\n",
        "    :param chunk_size: Max number of words per chunk\n",
        "    :param overlap: Number of overlapping words between chunks\n",
        "    :return: List of chunked text segments\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "\n",
        "    while start < len(words):\n",
        "        end = min(start + chunk_size, len(words))\n",
        "        chunk = words[start:end]\n",
        "        chunks.append(\" \".join(chunk))\n",
        "        start += chunk_size - overlap  # Slide the window\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Clean and split the document\n",
        "cleaned_text = clean_text(all_text)\n",
        "chunks = split_into_chunks(cleaned_text, chunk_size=500, overlap=50)\n",
        "\n",
        "# Display some chunks\n",
        "print(f\"Total Chunks Created: {len(chunks)}\\n\")\n",
        "print(f\"Sample Chunk [0]:\\n{chunks[0][:1000]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn0yUAyLj8zN",
        "outputId": "e4fff724-9f59-43e2-e95f-f6a6e84009e3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Chunks Created: 22\n",
            "\n",
            "Sample Chunk [0]:\n",
            "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks Patrick Lewis†‡, Ethan Perez⋆, Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†, Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela† †Facebook AI Research; ‡University College London; ⋆New York University; plewis@fb.com Abstract Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-speciﬁc architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre- trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Creating Embeddings and FAISS Vector Store\n",
        "\n",
        "We use the `SentenceTransformers` library to convert each text chunk into a high-dimensional vector (embedding) that captures its semantic meaning. These embeddings are stored in a FAISS index to enable fast approximate nearest neighbor search — the backbone of our document retriever.\n"
      ],
      "metadata": {
        "id": "f7OOdnjckNDj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0d3eddd"
      },
      "source": [
        "!pip install -q langchain_community"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding & Vector Store (LangChain FAISS)\n",
        "\n",
        "# Install the necessary packages (if not already done)\n",
        "!pip install -q faiss-cpu sentence-transformers langchain\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Wrap each chunk into a LangChain Document\n",
        "documents = [Document(page_content=chunk) for chunk in chunks]\n",
        "\n",
        "# Use the same embedding model we used earlier\n",
        "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Build the FAISS index using LangChain's wrapper (handles docstore + IDs)\n",
        "vectorstore = FAISS.from_documents(documents, embedding_function)\n",
        "\n",
        "# Confirm successful build\n",
        "print(f\"✅ LangChain-compatible FAISS index created with {len(documents)} documents.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq7SYaMiK63d",
        "outputId": "4f2c67a5-9461-4ad3-fef6-243d0310ebe5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3354598287.py:14: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LangChain-compatible FAISS index created with 22 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Integrating a Language Model with the Vector Store (RAG Pipeline)\n",
        "\n",
        "In this step, we build a complete Retrieval-Augmented Generation (RAG) pipeline using LangChain. This includes:\n",
        "- Connecting a language model (LLM) from Hugging Face\n",
        "- Wrapping our FAISS index in a retriever interface\n",
        "- Creating a RAG chain that retrieves relevant chunks and passes them to the LLM for contextual, grounded answers\n"
      ],
      "metadata": {
        "id": "4qCqzkD9lRlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Load a lightweight text2text model\n",
        "qa_pipeline = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    tokenizer=\"google/flan-t5-base\",\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "# Wrap the pipeline for LangChain\n",
        "llm = HuggingFacePipeline(pipeline=qa_pipeline)\n",
        "\n",
        "# Build the RAG-style RetrievalQA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "# Run a test query\n",
        "query = \"What is RAG-Token and how does it differ from RAG-Sequence?\"\n",
        "result = qa_chain.invoke(query)  # ✅ .invoke instead of deprecated __call__\n",
        "\n",
        "# Display the output\n",
        "print(\"🔍 Question:\", query)\n",
        "print(\"🧠 RAG Answer:\\n\", result['result'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRanHkKXLW-9",
        "outputId": "51f2ef81-2c37-438a-fade-9cd6fb580b7a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2730 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Question: What is RAG-Token and how does it differ from RAG-Sequence?\n",
            "🧠 RAG Answer:\n",
            " The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p′ (yi|x, y1:i1) = P ztop-k(p(|x)) p(zi|x) p(yi|x, zi, y1:i1) To decode, we can plug p′ (yi|x, y1:i1) into a standard beam decoder. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not find beam search improved results. For Open-Domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to find matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q, a) pair separately, leading to a small increase in accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Prompt Engineering and Comparative Evaluation\n",
        "\n",
        "In this step, we evaluate how retrieval improves generative QA. We test the same question on two systems:\n",
        "- ❌ A generic LLM with no access to source documents (pure parametric knowledge)\n",
        "- ✅ A RAG system that retrieves context from a document index before generating an answer\n",
        "\n",
        "This helps us observe the impact of retrieval grounding on the quality, specificity, and accuracy of answers.\n"
      ],
      "metadata": {
        "id": "zlBUeQY-muqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Non-RAG: Plain model with no access to documents\n",
        "generic_llm = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    tokenizer=\"google/flan-t5-base\",\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "# Same test question\n",
        "question = \"What is RAG-Token and how does it differ from RAG-Sequence?\"\n",
        "\n",
        "# Generate generic (non-grounded) response\n",
        "generic_response = generic_llm(question)[0]['generated_text']\n",
        "\n",
        "# Generate RAG-based (document-grounded) response\n",
        "rag_response = qa_chain.invoke(question)['result']\n",
        "\n",
        "# Compare outputs\n",
        "print(\"❌ Generic LLM Response:\\n\", generic_response)\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "print(\"✅ RAG-Grounded Response:\\n\", rag_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVD0TqUDPTW8",
        "outputId": "ba5f9356-d92a-4946-8eaa-e06cf87d1b85"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Generic LLM Response:\n",
            " RAG-Token\n",
            "\n",
            "================================================================================\n",
            "\n",
            "✅ RAG-Grounded Response:\n",
            " The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p′ (yi|x, y1:i1) = P ztop-k(p(|x)) p(zi|x) p(yi|x, zi, y1:i1) To decode, we can plug p′ (yi|x, y1:i1) into a standard beam decoder. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not find beam search improved results. For Open-Domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to find matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q, a) pair separately, leading to a small increase in accuracy.\n"
          ]
        }
      ]
    }
  ]
}